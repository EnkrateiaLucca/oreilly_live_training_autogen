{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"./autogen/OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-3.5-turbo\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_case1_web_info_task(web_task_query):\n",
    "    llm_config={\n",
    "    \"request_timeout\": 600,\n",
    "    \"seed\": 42,\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0,\n",
    "    }\n",
    "    assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,)\n",
    "\n",
    "    user_proxy = autogen.UserProxyAgent(\n",
    "        name=\"web_looker\",\n",
    "        human_input_mode=\"TERMINATE\",\n",
    "        max_consecutive_auto_reply=5,\n",
    "        is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "        code_execution_config={\"work_dir\": \"web\"},\n",
    "        llm_config=llm_config,\n",
    "        system_message=\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.\n",
    "    Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\"\n",
    "    )\n",
    "\n",
    "    # the assistant receives a message from the user, which contains the task description\n",
    "    user_proxy.initiate_chat(\n",
    "        assistant,\n",
    "        message=f\"\"\"\n",
    "    {web_task_query}\n",
    "    \"\"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Successful prompts for the web info task\n",
    "# 1. Reading through arxiv papers\n",
    "# use_case1_web_info_task(\"Who should read this paper: https://arxiv.org/abs/2308.08155\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the teaching use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_case2_research_workflows(tasks_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creating research workflows from a list of tasks.\n",
    "    task1 = 'Find arxiv papers that show comparisons betweewn how humans and machines learn the same tasks. '\n",
    "\n",
    "    user_proxy.initiate_chat(assistant, message=task1)\n",
    "\n",
    "    task2 = \"Analyse the above papers mentioned and find out a list of interesting and original insights about similarities between human and machine learning.\"\n",
    "\n",
    "    user_proxy.initiate_chat(assistant, message=task2, clear_history=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    llm_config = {\n",
    "    \"request_timeout\":600,\n",
    "    \"seed\":44,\n",
    "    \"config_list\":config_list,\n",
    "    \"temperature\":0\n",
    "    }\n",
    "\n",
    "    assistant = autogen.AssistantAgent(\n",
    "        name=\"assistant\", \n",
    "        llm_config=llm_config,\n",
    "        is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    "    )\n",
    "\n",
    "    user_proxy = autogen.UserProxyAgent(\n",
    "        name=\"user_proxy\",\n",
    "        human_input_mode=\"TERMINATE\",\n",
    "        is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    "        max_consecutive_auto_reply=10,\n",
    "        code_execution_config={\"work_dir\": \"work_dir\",\n",
    "                            \"use_docker\": False,\n",
    "                            },\n",
    "    )\n",
    "    \n",
    "    for task in tasks_list:\n",
    "        user_proxy.initiate_chat(assistant, message=task, clear_history=False)\n",
    "    \n",
    "    \n",
    "    task_recipe_create =  '''Reflect on the sequence and create a recipe containing all the steps \n",
    "                                # necessary and name for it. Suggest well-documented, generalized python function(s)\n",
    "                                #  to perform similar tasks for coding steps in future. Make sure coding steps and \n",
    "                                #  non-coding steps are never mixed in one function. In the docstr of the function(s),\n",
    "                                #  clarify what non-coding steps are needed to use the language skill of the assistant.\n",
    "                          '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-usable recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from the agentchat_teaching.ipynb notebook in the Autogen github repo:\n",
    "\n",
    "```\n",
    "\n",
    "# create an AssistantAgent instance named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"work_dir\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "task1 = '''\n",
    "This recipe is available for you to reuse..\n",
    "\n",
    "<begin recipe>\n",
    "**Recipe Name:** Analyzing and Visualizing Application Domains in Arxiv Papers\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Collect relevant papers from arxiv using a search query.\n",
    "2. Analyze the abstracts of the collected papers to identify application domains.\n",
    "3. Count the number of papers in each application domain.\n",
    "4. Generate a bar chart of the application domains and the number of papers in each domain.\n",
    "5. Save the bar chart as an image file.\n",
    "\n",
    "Here are the well-documented, generalized Python functions to perform the coding steps in the future:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import feedparser\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "\n",
    "def search_arxiv(query: str, max_results: int = 10) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Search arxiv for papers related to a specific query.\n",
    "    \n",
    "    :param query: The search query for arxiv papers.\n",
    "    :param max_results: The maximum number of results to return. Default is 10.\n",
    "    :return: A list of dictionaries containing the title, link, and summary of each paper.\n",
    "    \"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    search_query = f\"search_query=all:{query}\"\n",
    "    start = 0\n",
    "    max_results = f\"max_results={max_results}\"\n",
    "    url = f\"{base_url}{search_query}&start={start}&{max_results}\"\n",
    "    response = requests.get(url)\n",
    "    feed = feedparser.parse(response.content)\n",
    "    \n",
    "    papers = [{\"title\": entry.title, \"link\": entry.link, \"summary\": entry.summary} for entry in feed.entries]\n",
    "    return papers\n",
    "\n",
    "def generate_bar_chart(domains: Dict[str, int], output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate a bar chart of application domains and the number of papers in each domain, and save it as an image file.\n",
    "    \n",
    "    :param domains: A dictionary containing application domains as keys and the number of papers as values.\n",
    "    :param output_file: The name of the output image file.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(domains.keys(), domains.values())\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Application Domains\")\n",
    "    plt.ylabel(\"Number of Papers\")\n",
    "    plt.title(\"Number of Papers per Application Domain\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "**Usage:**\n",
    "\n",
    "1. Use the `search_arxiv` function to collect relevant papers from arxiv using a search query.\n",
    "2. Analyze the abstracts of the collected papers using your language skills to identify application domains and count the number of papers in each domain.\n",
    "3. Use the `generate_bar_chart` function to generate a bar chart of the application domains and the number of papers in each domain, and save it as an image file.\n",
    "\n",
    "</end recipe>\n",
    "\n",
    "\n",
    "Here is a new task:\n",
    "Plot a chart for application domains of GPT models\n",
    "'''\n",
    "\n",
    "user_proxy.initiate_chat(assistant, message=task1)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://lightning.ai/docs\"\n",
    "prompt = f\"\"\"\"\"\"\n",
    "use_case1_web_info_task(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "autogen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
