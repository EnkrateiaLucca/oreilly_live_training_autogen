{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Please install `ollama` to use the Ollama API.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# The AssistantAgent, using the Ollama config, will take the coding request and return code\u001b[39;00m\n\u001b[1;32m     34\u001b[0m config_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     35\u001b[0m     {\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     }\n\u001b[1;32m     41\u001b[0m ]\n\u001b[0;32m---> 43\u001b[0m assistant_agent \u001b[38;5;241m=\u001b[39m \u001b[43mAssistantAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOllama Assistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_list\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Start the chat\u001b[39;00m\n\u001b[1;32m     50\u001b[0m chat_result \u001b[38;5;241m=\u001b[39m user_proxy_agent\u001b[38;5;241m.\u001b[39minitiate_chat(\n\u001b[1;32m     51\u001b[0m     assistant_agent,\n\u001b[1;32m     52\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvide code to count the number of prime numbers from 1 to 10000.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-autogen/lib/python3.11/site-packages/autogen/agentchat/assistant_agent.py:68\u001b[0m, in \u001b[0;36mAssistantAgent.__init__\u001b[0;34m(self, name, system_message, llm_config, is_termination_msg, max_consecutive_auto_reply, human_input_mode, description, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     42\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     50\u001b[0m ):\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m        name (str): agent name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m            [ConversableAgent](conversable_agent#__init__).\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_termination_msg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_consecutive_auto_reply\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhuman_input_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m     79\u001b[0m         log_new_agent(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlocals\u001b[39m())\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:170\u001b[0m, in \u001b[0;36mConversableAgent.__init__\u001b[0;34m(self, name, system_message, is_termination_msg, max_consecutive_auto_reply, human_input_mode, function_map, code_execution_config, llm_config, default_auto_reply, description, chat_messages, silent)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease implement __deepcopy__ method for each value class in llm_config to support deepcopy.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Refer to the docs for more details: https://autogen-ai.github.io/autogen/docs/topics/llm_configuration#adding-http-client-in-llm_config-for-proxy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_llm_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m    173\u001b[0m     log_new_agent(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlocals\u001b[39m())\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:274\u001b[0m, in \u001b[0;36mConversableAgent._validate_llm_config\u001b[0;34m(self, llm_config)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config \u001b[38;5;129;01min\u001b[39;00m [{}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m}]}]:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using OpenAI or Azure OpenAI endpoints, specify a non-empty \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m either in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_config\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or in each config of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     )\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mOpenAIWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-autogen/lib/python3.11/site-packages/autogen/oai/client.py:442\u001b[0m, in \u001b[0;36mOpenAIWrapper.__init__\u001b[0;34m(self, config_list, **base_config)\u001b[0m\n\u001b[1;32m    440\u001b[0m     config_list \u001b[38;5;241m=\u001b[39m [config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m config_list]  \u001b[38;5;66;03m# make a copy before modifying\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m config_list:\n\u001b[0;32m--> 442\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_default_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenai_config\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# could modify the config\u001b[39;00m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    444\u001b[0m             {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_kwargs}}\n\u001b[1;32m    445\u001b[0m         )\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/oreilly-autogen/lib/python3.11/site-packages/autogen/oai/client.py:547\u001b[0m, in \u001b[0;36mOpenAIWrapper._register_default_client\u001b[0;34m(self, config, openai_config)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m api_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m api_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ollama_import_exception:\n\u001b[0;32m--> 547\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install `ollama` to use the Ollama API.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    548\u001b[0m     client \u001b[38;5;241m=\u001b[39m OllamaClient(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopenai_config)\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clients\u001b[38;5;241m.\u001b[39mappend(client)\n",
      "\u001b[0;31mImportError\u001b[0m: Please install `ollama` to use the Ollama API."
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "# Setting up the code executor\n",
    "workdir = Path(\"coding\")\n",
    "workdir.mkdir(exist_ok=True)\n",
    "code_executor = LocalCommandLineCodeExecutor(work_dir=workdir)\n",
    "\n",
    "# Setting up the agents\n",
    "user_proxy_agent = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    code_execution_config={\"executor\": code_executor},\n",
    "    is_termination_msg=lambda msg: \"FINISH\" in msg.get(\"content\"),\n",
    ")\n",
    "\n",
    "system_message = \"\"\"You are a helpful AI assistant who writes code and the user\n",
    "executes it. Solve tasks using your python coding skills.\n",
    "In the following cases, suggest python code (in a python coding block) for the\n",
    "user to execute. When using code, you must indicate the script type in the code block.\n",
    "You only need to create one working sample.\n",
    "Do not suggest incomplete code which requires users to modify it.\n",
    "Don't use a code block if it's not intended to be executed by the user. Don't\n",
    "include multiple code blocks in one response. Do not ask users to copy and\n",
    "paste the result. Instead, use 'print' function for the output when relevant.\n",
    "Check the execution result returned by the user.\n",
    "\n",
    "If the result indicates there is an error, fix the error.\n",
    "\n",
    "IMPORTANT: If it has executed successfully, ONLY output 'FINISH'.\"\"\"\n",
    "\n",
    "# The AssistantAgent, using the Ollama config, will take the coding request and return code\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"llama3.2\",\n",
    "        \"api_type\": \"ollama\",\n",
    "        \"stream\": False,\n",
    "        \"client_host\": \"http://192.168.0.1:11434\",\n",
    "    }\n",
    "]\n",
    "\n",
    "assistant_agent = AssistantAgent(\n",
    "    name=\"Ollama Assistant\",\n",
    "    system_message=system_message,\n",
    "    llm_config={\"config_list\": config_list},\n",
    ")\n",
    "\n",
    "# Start the chat\n",
    "chat_result = user_proxy_agent.initiate_chat(\n",
    "    assistant_agent,\n",
    "    message=\"Provide code to count the number of prime numbers from 1 to 10000.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool Calling Example\n",
    "import json\n",
    "from typing import Literal\n",
    "from typing_extensions import Annotated\n",
    "import autogen\n",
    "\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"llama3.1:8b\",\n",
    "        \"api_type\": \"ollama\",\n",
    "        \"stream\": False,\n",
    "        \"client_host\": \"http://192.168.0.1:11434\",\n",
    "        \"hide_tools\": \"if_any_run\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create the agent\n",
    "chatbot = autogen.AssistantAgent(\n",
    "    name=\"chatbot\",\n",
    "    system_message=\"\"\"For currency exchange and weather forecasting tasks,\n",
    "        only use the functions you have been provided with.\n",
    "        Example of the return JSON is:\n",
    "        {\n",
    "            \"parameter_1_name\": 100.00,\n",
    "            \"parameter_2_name\": \"ABC\",\n",
    "            \"parameter_3_name\": \"DEF\",\n",
    "        }.\n",
    "        Another example of the return JSON is:\n",
    "        {\n",
    "            \"parameter_1_name\": \"GHI\",\n",
    "            \"parameter_2_name\": \"ABC\",\n",
    "            \"parameter_3_name\": \"DEF\",\n",
    "            \"parameter_4_name\": 123.00,\n",
    "        }.\n",
    "        Output 'HAVE FUN!' when an answer has been provided.\"\"\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and \"HAVE FUN!\" in x.get(\"content\", \"\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=1,\n",
    ")\n",
    "\n",
    "# Currency Exchange function\n",
    "CurrencySymbol = Literal[\"USD\", \"EUR\"]\n",
    "\n",
    "def exchange_rate(base_currency: CurrencySymbol, quote_currency: CurrencySymbol) -> float:\n",
    "    if base_currency == quote_currency:\n",
    "        return 1.0\n",
    "    elif base_currency == \"USD\" and quote_currency == \"EUR\":\n",
    "        return 1 / 1.1\n",
    "    elif base_currency == \"EUR\" and quote_currency == \"USD\":\n",
    "        return 1.1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown currencies {base_currency}, {quote_currency}\")\n",
    "\n",
    "@user_proxy.register_for_execution()\n",
    "@chatbot.register_for_llm(description=\"Currency exchange calculator.\")\n",
    "def currency_calculator(\n",
    "    base_amount: Annotated[\n",
    "        float,\n",
    "        \"Amount of currency in base_currency. Type is float, not string, return value should be a number only, e.g. 987.65.\",\n",
    "    ],\n",
    "    base_currency: Annotated[CurrencySymbol, \"Base currency\"] = \"USD\",\n",
    "    quote_currency: Annotated[CurrencySymbol, \"Quote currency\"] = \"EUR\",\n",
    ") -> str:\n",
    "    quote_amount = exchange_rate(base_currency, quote_currency) * base_amount\n",
    "    return f\"{format(quote_amount, '.2f')} {quote_currency}\"\n",
    "\n",
    "# Weather function\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the weather for some location\"\"\"\n",
    "    if \"chicago\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Chicago\", \"temperature\": \"13\", \"unit\": unit})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"55\", \"unit\": unit})\n",
    "    elif \"new york\" in location.lower():\n",
    "        return json.dumps({\"location\": \"New York\", \"temperature\": \"11\", \"unit\": unit})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n",
    "@user_proxy.register_for_execution()\n",
    "@chatbot.register_for_llm(description=\"Weather forecast for US cities.\")\n",
    "def weather_forecast(\n",
    "    location: Annotated[str, \"City name\"],\n",
    ") -> str:\n",
    "    weather_details = get_current_weather(location=location)\n",
    "    weather = json.loads(weather_details)\n",
    "    return f\"{weather['location']} will be {weather['temperature']} degrees {weather['unit']}\"\n",
    "\n",
    "# Start the conversation\n",
    "res = user_proxy.initiate_chat(\n",
    "    chatbot,\n",
    "    message=\"What's the weather in New York and can you tell me how much is 123.45 EUR in USD so I can spend it on my holiday? Throw a few holiday tips in as well.\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")\n",
    "\n",
    "print(f\"LLM SUMMARY: {res.summary['content']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-autogen",
   "language": "python",
   "name": "oreilly-autogen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
