{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbec5bc2",
   "metadata": {},
   "source": [
    "# RAG Agent\n",
    "\n",
    "The RAG (Retrieval Augmented Generation):\n",
    "\n",
    "1. Indexing: Loading documents, chunking them, and storing them in a vector database\n",
    "\n",
    "2. Retrieval: Finding and using relevant chunks during conversation runtime\n",
    "\n",
    "Note: The quality of a RAG system depends on factors like chunking and retrieval (models, embeddings, etc.). You may have to experiment with those to get the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d23e4",
   "metadata": {},
   "source": [
    "## Simple RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de7dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html#:~:text=To%20begin%2C%20let%E2%80%99s%20create%20a%20simple%20document%20indexer%20that%20we%20will%20used%20to%20load%20documents%2C%20chunk%20them%2C%20and%20store%20them%20in%20a%20ChromaDBVectorMemory%20memory%20store.\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import aiofiles\n",
    "import aiohttp\n",
    "from autogen_core.memory import Memory, MemoryContent, MemoryMimeType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f5fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDocumentIndexer:\n",
    "    \"\"\"Basic document indexer for AutoGen Memory.\"\"\"\n",
    "\n",
    "    def __init__(self, memory: Memory, chunk_size: int = 1500) -> None:\n",
    "        self.memory = memory\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    async def _fetch_content(self, source: str) -> str:\n",
    "        \"\"\"Fetch content from URL or file.\"\"\"\n",
    "        if source.startswith((\"http://\", \"https://\")):\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(source) as response:\n",
    "                    return await response.text()\n",
    "        else:\n",
    "            async with aiofiles.open(source, \"r\", encoding=\"utf-8\") as f:\n",
    "                return await f.read()\n",
    "\n",
    "    def _strip_html(self, text: str) -> str:\n",
    "        \"\"\"Remove HTML tags and normalize whitespace.\"\"\"\n",
    "        text = re.sub(r\"<[^>]*>\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "    def _split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into fixed-size chunks.\"\"\"\n",
    "        chunks: list[str] = []\n",
    "        # Just split text into fixed-size chunks\n",
    "        for i in range(0, len(text), self.chunk_size):\n",
    "            chunk = text[i : i + self.chunk_size]\n",
    "            chunks.append(chunk.strip())\n",
    "        return chunks\n",
    "\n",
    "    async def index_documents(self, sources: List[str]) -> int:\n",
    "        \"\"\"Index documents into memory.\"\"\"\n",
    "        total_chunks = 0\n",
    "\n",
    "        for source in sources:\n",
    "            try:\n",
    "                content = await self._fetch_content(source)\n",
    "\n",
    "                # Strip HTML if content appears to be HTML\n",
    "                if \"<\" in content and \">\" in content:\n",
    "                    content = self._strip_html(content)\n",
    "\n",
    "                chunks = self._split_text(content)\n",
    "\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    await self.memory.add(\n",
    "                        MemoryContent(\n",
    "                            content=chunk, mime_type=MemoryMimeType.TEXT, metadata={\"source\": source, \"chunk_index\": i}\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                total_chunks += len(chunks)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error indexing {source}: {str(e)}\")\n",
    "\n",
    "        return total_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7d05e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 40 chunks from 9 AutoGen documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Initialize vector memory\n",
    "\n",
    "rag_memory = ChromaDBVectorMemory(\n",
    "    config=PersistentChromaDBVectorMemoryConfig(\n",
    "        collection_name=\"autogen_docs\",\n",
    "        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),\n",
    "        k=3,  # Return top 3 results\n",
    "        score_threshold=0.4,  # Minimum similarity score\n",
    "    )\n",
    ")\n",
    "\n",
    "await rag_memory.clear()  # Clear existing memory\n",
    "\n",
    "\n",
    "# Index AutoGen documentation\n",
    "indexer = SimpleDocumentIndexer(memory=rag_memory)\n",
    "sources = [\n",
    "    \"https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html\",\n",
    "    \"https://microsoft.github.io/autogen/autogen/user-guide/autogenstudio-user-guide/index.html\",\n",
    "    \"https://microsoft.github.io/autogen/autogen/user-guide/agentchat-user-guide/index.html\",\n",
    "    \"https://microsoft.github.io/autogen/autogen/user-guide/core-user-guide/index.html\",\n",
    "    \"https://microsoft.github.io/autogen/autogen/user-guide/extensions-user-guide/index.html\",\n",
    "    \"https://microsoft.github.io/autogen/autogen/reference/index.html\",\n",
    "    \"https://microsoft.github.io/autogen/autogen/user-guide/agentchat-user-guide/migration-guide.html\",\n",
    "    \"https://microsoft.github.io/autogen/autogen/user-guide/agentchat-user-guide/quickstart.html\",\n",
    "    \"https://microsoft.github.io/autogen/autogen/user-guide/core-user-guide/quickstart.html\",\n",
    "]\n",
    "chunks: int = await indexer.index_documents(sources)\n",
    "print(f\"Indexed {chunks} chunks from {len(sources)} AutoGen documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa99bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to build a rag agent in the autogen framework?\n",
      "---------- MemoryQueryEvent (rag_assistant) ----------\n",
      "[MemoryContent(content='our indexer with ChromaDBVectorMemory to build a complete RAG agent: import os from pathlib import Path from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console from autogen_ext.memory.chromadb import ChromaDBVectorMemory , PersistentChromaDBVectorMemoryConfig from autogen_ext.models.openai import OpenAIChatCompletionClient # Initialize vector memory rag_memory = ChromaDBVectorMemory ( config = PersistentChromaDBVectorMemoryConfig ( collection_name = &quot;autogen_docs&quot; , persistence_path = os . path . join ( str ( Path . home ()), &quot;.chromadb_autogen&quot; ), k = 3 , # Return top 3 results score_threshold = 0.4 , # Minimum similarity score ) ) await rag_memory . clear () # Clear existing memory # Index AutoGen documentation async def index_autogen_docs () -&gt; None : indexer = SimpleDocumentIndexer ( memory = rag_memory ) sources = [ &quot;https://raw.githubusercontent.com/microsoft/autogen/main/README.md&quot; , &quot;https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html&quot; , &quot;https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/teams.html&quot; , &quot;https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/termination.html&quot; , ] chunks : int = await indexer . index_documents ( sources ) print ( f &quot;Indexed { chunks } chunks from { len ( sources ) } AutoGen documents&quot; ) await index_autogen_docs () Indexed 70 chunks f', mime_type='MemoryMimeType.TEXT', metadata={'mime_type': 'MemoryMimeType.TEXT', 'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html', 'chunk_index': 16, 'score': 0.49464482069015503, 'id': '834c7810-dd51-45bf-9b74-5b4e7199f34a'}), MemoryContent(content='rom 4 AutoGen documents # Create our RAG assistant agent rag_assistant = AssistantAgent ( name = &quot;rag_assistant&quot; , model_client = OpenAIChatCompletionClient ( model = &quot;gpt-4o&quot; ), memory = [ rag_memory ] ) # Ask questions about AutoGen stream = rag_assistant . run_stream ( task = &quot;What is AgentChat?&quot; ) await Console ( stream ) # Remember to close the memory when done await rag_memory . close () ---------- TextMessage (user) ---------- What is AgentChat? ---------- MemoryQueryEvent (rag_assistant) ---------- [MemoryContent(content=&#39;e of the AssistantAgent , we can now proceed to the next section to learn about the teams feature in AgentChat. previous Messages next Teams On this page Assistant Agent Getting Result Multi-Modal Input Streaming Messages Using Tools and Workbench Built-in Tools and Workbench Function Tool Model Context Protocol (MCP) Workbench Agent as a Tool Parallel Tool Calls Tool Iterations Structured Output Streaming Tokens Using Model Context Other Preset Agents Next Step Edit on GitHub Show Source so the DOM is not blocked --&gt; © Copyright 2024, Microsoft. Privacy Policy | Consumer Health Privacy Built with the PyData Sphinx Theme 0.16.0.&#39;, mime_type=&#39;MemoryMimeType.TEXT&#39;, metadata={&#39;chunk_index&#39;: 16, &#39;mime_type&#39;: &#39;MemoryMimeType.TEXT&#39;, &#39;source&#39;: &#39;https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html&#39;, &#39;score&#39;: 0.6237251460552', mime_type='MemoryMimeType.TEXT', metadata={'mime_type': 'MemoryMimeType.TEXT', 'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html', 'chunk_index': 17, 'score': 0.4912238121032715, 'id': '8e8bc574-91aa-48bb-863a-b44ac7bbaee6'}), MemoryContent(content='rch Literature Review API Reference PyPi Source AgentChat Memory and RAG Memory and RAG # There are several use cases where it is valuable to maintain a store of useful facts that can be intelligently added to the context of the agent just before a specific step. The typically use case here is a RAG pattern where a query is used to retrieve relevant information from a database that is then added to the agent’s context. AgentChat provides a Memory protocol that can be extended to provide this functionality. The key methods are query , update_context , add , clear , and close . add : add new entries to the memory store query : retrieve relevant information from the memory store update_context : mutate an agent’s internal model_context by adding the retrieved information (used in the AssistantAgent class) clear : clear all entries from the memory store close : clean up any resources used by the memory store ListMemory Example # ListMemory is provided as an example implementation of the Memory protocol. It is a simple list-based memory implementation that maintains memories in chronological order, appending the most recent memories to the model’s context. The implementation is designed to be straightforward and predictable, making it easy to understand and debug. In the following example, we will use ListMemory to maintain a memory bank of user preferences and demonstrate how it can be used to provide consistent context for agent responses over time. from autogen_agentchat.agents', mime_type='MemoryMimeType.TEXT', metadata={'chunk_index': 1, 'mime_type': 'MemoryMimeType.TEXT', 'source': 'https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html', 'score': 0.40787273645401, 'id': '6cfe25d9-f6c1-44ab-a1a9-f9a13f352fd8'})]\n",
      "---------- TextMessage (rag_assistant) ----------\n",
      "To build a Retrieval-Augmented Generation (RAG) agent using the AutoGen framework, you will need to set up a vector memory system and initialize an assistant agent that can utilize this memory. Here's a step-by-step guide to help you get started:\n",
      "\n",
      "1. **Import Required Modules:**\n",
      "    You need several modules from the AutoGen framework, including those for memory, agent creation, and model client interaction.\n",
      "\n",
      "    ```python\n",
      "    import os\n",
      "    from pathlib import Path\n",
      "    from autogen_agentchat.agents import AssistantAgent\n",
      "    from autogen_agentchat.ui import Console\n",
      "    from autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\n",
      "    from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
      "    ```\n",
      "\n",
      "2. **Initialize Vector Memory:**\n",
      "    You need to set up a vector memory system using `ChromaDBVectorMemory`. This memory will store and retrieve relevant information that can aid in generating responses.\n",
      "\n",
      "    ```python\n",
      "    rag_memory = ChromaDBVectorMemory(\n",
      "        config=PersistentChromaDBVectorMemoryConfig(\n",
      "            collection_name=\"autogen_docs\",\n",
      "            persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),\n",
      "            k=3,  # Return top 3 results\n",
      "            score_threshold=0.4,  # Minimum similarity score\n",
      "        )\n",
      "    )\n",
      "    await rag_memory.clear()  # Clear any existing memory\n",
      "    ```\n",
      "\n",
      "3. **Index Documents:**\n",
      "    Use a document indexer to index the documents you want the RAG agent to retrieve information from. This typically involves fetching documents and breaking them into chunks that can be stored in the memory.\n",
      "\n",
      "    ```python\n",
      "    async def index_autogen_docs() -> None:\n",
      "        indexer = SimpleDocumentIndexer(memory=rag_memory)\n",
      "        sources = [\n",
      "            \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n",
      "            \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html\",\n",
      "            \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/teams.html\",\n",
      "            \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/termination.html\",\n",
      "        ]\n",
      "        chunks = await indexer.index_documents(sources)\n",
      "        print(f\"Indexed {chunks} chunks from {len(sources)} AutoGen documents\")\n",
      "\n",
      "    await index_autogen_docs()\n",
      "    ```\n",
      "\n",
      "4. **Create the RAG Assistant Agent:**\n",
      "    Initialize an assistant agent that can use the indexed vector memory to answer queries. This agent uses a model client like `OpenAIChatCompletionClient` to generate responses.\n",
      "\n",
      "    ```python\n",
      "    rag_assistant = AssistantAgent(\n",
      "        name=\"rag_assistant\",\n",
      "        model_client=OpenAIChatCompletionClient(model=\"gpt-4o\"),\n",
      "        memory=[rag_memory]\n",
      "    )\n",
      "    ```\n",
      "\n",
      "5. **Responsiveness with the Console Interface:**\n",
      "    You can ask questions to your RAG agent and view responses through a streaming console interface.\n",
      "\n",
      "    ```python\n",
      "    stream = rag_assistant.run_stream(task=\"What is AgentChat?\")\n",
      "    await Console(stream)\n",
      "    ```\n",
      "\n",
      "6. **Closing the Memory:**\n",
      "    Once you are done, remember to clean up resources by closing the memory.\n",
      "\n",
      "    ```python\n",
      "    await rag_memory.close()\n",
      "    ```\n",
      "\n",
      "This setup configures a RAG agent capable of answering questions by retrieving and using context from pre-indexed documents, enhancing the quality and specificity of its responses.\n"
     ]
    }
   ],
   "source": [
    "# Create our RAG assistant agent\n",
    "rag_assistant = AssistantAgent(\n",
    "    name=\"rag_assistant\", model_client=OpenAIChatCompletionClient(model=\"gpt-4o\"), memory=[rag_memory]\n",
    ")\n",
    "\n",
    "# Ask questions about AutoGen\n",
    "stream = rag_assistant.run_stream(task=\"How to build a rag agent in the autogen framework?\")\n",
    "await Console(stream)\n",
    "\n",
    "# Remember to close the memory when done\n",
    "await rag_memory.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376e7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0ec3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
