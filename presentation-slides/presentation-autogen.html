<!DOCTYPE html>
<html>
  <head>
    <title>Getting Started with AutoGen</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

        class: center, middle

        # Getting Started with AutoGen
        ### By Lucas Soares

        ---
        <div class="slide">
          <h1>Lucas Soares</h1>
          <div style="display: flex; align-items: center;">
            <ul style="flex: 1;">
              <li>ML Engineer</li>
              <br>
              <br>
              <br>
              <br>
            </ul>
            <img src="../notebooks/assets-resources/profile_pic.png" width="300" style="margin-left: 20px;">
          </div>
        </div>
        ---

        <div class="slide">
          <h1>Lucas Soares</h1>
          <div style="display: flex; align-items: center;">
            <ul style="flex: 1;">
              <li>ML Engineer</li>
              <br>
              <li>Instructor at O'Reilly Media</li>
              <br>
              <br>
            </ul>
            <img src="../notebooks/assets-resources/profile_pic.png" width="300" style="margin-left: 20px;">
          </div>
        </div>

        ---

        <div class="slide">
          <h1>Lucas Soares</h1>
          <div style="display: flex; align-items: center;">
            <ul style="flex: 1;">
              <li>ML Engineer</li>
              <br>
              <li>Instructor at O'Reilly Media</li>
              <br>
              <li>Curious about all things intelligence</li>
            </ul>
            <img src="../notebooks/assets-resources/profile_pic.png" width="300" style="margin-left: 20px;">
          </div>
        </div>

        ---

        # Table of Contents

        - __Agents as Thought + Action__
        
        --

        - __Agents:__ __Definition & Tools__

        --

        - __Agents in 3 Complexity Levels__

        --

        - __OpenAI's Function API__

        --
        
        - __AutoGen Framework__

        --

        - __Building Agents with AutoGen__

        --

        - __Lots of Demos! :)__

        ---

        # Thought + Action

        --

        - How do we do stuff?

        ---

        # Thought + Action

        - How do we do stuff? We __think__ and we __act__
        
        --

        - Example: Decision-making process for attending a live-training

        --

        - __Thought__: "I want to learn about agents"
        
        --
        
        - __Action__: "Go to the internet and research cool 
        platforms where I can learn about agents"
        
        --

        - __Thought__: "O'Reilly has some awesome courses and live-trainings"
        
        --
        
        - __Action__: "Look up O'Reilly courses"
        
        --
        
        - __Thought__: "Live-trainings by instructor Lucas are awesome"
        
        --
        
        - __Action__: "Schedule live-training about agents with instructor Lucas Soares"

        ---
        class: center, middle

        # Thinking: 
        
        
        ## What to do + planning (order, priority..)
        
        ---
        class: center, middle

        # Acting: 
        
        
        ## used __tools: search, browser, etc...__

        ---

        # What is an Agent?

        --

        ## LLM

        ---

        # What is an Agent?


        ## LLM + Tools

        --

        <div style="display: flex; align-items: center;">
          <h4>LLM</h4><p> = predicts next word/sentence</p>
          <img src="../notebooks/assets-resources/llm_predicts_pancakes.png" width="300" style="margin-left:100px">
        </div>

        --

        <div style="display: flex; align-items: center;">
          <h4>Tool</h4><p> = perform actions in the real-world</p>
          <img src="../notebooks/assets-resources/pancake_maker.png" width="300" style="margin-left: 50px;">
        </div>


        ---
        
        # ['Toolformer'](https://arxiv.org/pdf/2302.04761.pdf)

        --


        <li>LLMs can teach themselves how to <br>  
            properly call external tools!</li>

        <img style="width: 300px "src="../notebooks/assets-resources/toolformer.png">

        <div style="display: flex; align-items: center;">
          <li>Tool = </li>
          <img src="../notebooks/assets-resources/python_icon.png" width="50" style="margin-left: 10px;">
        </div>

        

        ---
        # [ReACT](https://arxiv.org/pdf/2210.03629.pdf)

        - LLMs for __RE__asoning & __ACT__ion.

        <img style="width: 600px" src="../notebooks/assets-resources/react_paper_figure.png" >

        ---
        
        ## Agents Are Getting Popular

        --

        - [A Survey on Large Language Model based
        Autonomous Agents](https://arxiv.org/pdf/2308.11432.pdf)  

        <img style="width: 600px" src="../notebooks/assets-resources/agents_growth_trends.png">

        ---
        # Popular Agent Implementations

        --
        
        - [BabyAGI](https://github.com/yoheinakajima/babyagi): separate planning and execution steps

        --

        - [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT): created for long-running, open-ended goals

        --

        - [GPT-Researcher](https://github.com/assafelovic/gpt-researcher?ref=blog.langchain.dev): produce detailed, factual and unbiased research reports

        --

        - [OpenGPTs](https://github.com/langchain-ai/opengpts?ref=blog.langchain.dev): Open Source Customizable Agents  
        ---

        ## Andrej Karpathy on Agents

        - Agents are cool!!!

        <iframe width="560" height="315" src="https://www.youtube.com/embed/fqVLjtvWgq8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

        ---
        class: center, middle
        # Agents in 3 Levels of Complexity

        ---

        # Level 1: LLM + functions inside the prompt 

        --
        
        - Inspired by ['Toolformer'](https://arxiv.org/pdf/2302.04761.pdf)

        <img style="width: 400px "src="../notebooks/assets-resources/toolformer.png"> </img>

        ---

        # Level 1: LLM + functions inside the prompt


        ```python
        from openai import OpenAI
        client = OpenAI()
        
        def get_response(prompt_question, model="gpt-3.5-turbo-16k"):
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "system", "content": "You are a helpful research and programming assistant"},
                        {"role": "user", "content": prompt_question}]
            )
            
            return response.choices[0].message.content

        def create_directory(directory_name):
            subprocess.run(["mkdir", directory_name])

        def create_file(file_name):
            subprocess.run(["touch", file_name])

        def list_files():
            subprocess.run(["ls"])
        ```

        ---

        # Level 1: LLM + functions inside the prompt

        ```python

          task_description = "Create a folder called 'lucas-the-agent-master'. Inside that folder, create a file called 'the-10-master-rules.md"
          output = get_response(f"""Given this task: {task_description}, \n
                                  Consider you have access to the following functions:
                                  
          def create_directory(directory_name):
              '''Function that creates a directory given a directory name.'''
              subprocess.run(["mkdir", directory_name])
          
          def create_file(file_name):
              '''Function that creates a file given a file name.'''
              subprocess.run(["touch", file_name])
          
          def list_files():
            '''Function that lists all files in the current directory.'''
              subprocess.run(["ls"])
          
          Your output should be the first function to be executed to complete the task containing the necessary arguments.
          The OUTPUT SHOULD ONLY BE THE PYTHON FUNCTION CALL and NOTHING ELSE.
          """)
          
          Markdown(output)

          # Output:
          # create_directory('lucas-the-agent-master')
        ```

        ---

        # Level 1: LLM + functions inside the prompt

        -  Now, all we need is to find a way to execute this function. 

        --

        - We can use Python's built in `exec` method for that:
        
        --
        ```python
        exec("model." + output)
        !ls -d */ | grep lucas
        # Output:
        # lucas-the-agent-master/
        ```
        
        ---

        # Limitations

        --

        - __Probabilistic outputs__ make function calls unreliable
        
        --

        - Need for __structured ways to prepare the inputs__ of 
        the function calls
        
        --
        
        - Putting entire functions inside text prompts is clunky and
        __non-scalable__
        
        --

        - Solution? __OpenAI Functions__!
        
        ---
        class: center, middle

        # Q&A
        
        ---
        class: center, middle

        # OpenAI's Function Calling API
        
        ---

        # OpenAI Function Calling

        --

        - [OpenAI function calling](https://platform.openai.com/docs/guides/function-calling):

        ---

        # OpenAI Function Calling


        - [OpenAI function calling](https://platform.openai.com/docs/guides/function-calling): standard way to connect models to outside tools.

        --

        ### Steps

        --

        1. Call the model with the user query and a set of functions defined in the functions parameter.
        
        --
        
        2. The model can choose to call one or more functions; if so, the content will be a stringified JSON object adhering to your custom schema.
        
        --
        
        3. Parse the string into JSON in your code, and call your function with the provided arguments if they exist.
        
        --
        
        4. Call the model again by appending the function response as a new message, and let the model summarize the results back to the user.
        

        ---

        # Step 1 & 2

        --
        
        ```python
        import json
        
        def create_directory(directory_name):
            # Function to create a directory
            subprocess.run(["mkdir", directory_name])
            return json.dumps({"directory_name": directory_name})
        
        tool_create_directory = {
            "type": "function",
            "function": {
                "name": "create_directory",
                "description": "Create a directory given a directory name.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "directory_name": {
                            "type": "string",
                            "description": "The name of the directory to create.",
                        }
                    },
                    "required": ["directory_name"],
                },
            },
        }
        tools = [tool_create_directory]

        ```

        ---

        # Step 1 & 2

        ```python

        def run_terminal_task():
            messages = [{"role": "user", "content": 
            "Create a folder called 'lucas-the-agent-master'."}]
            tools = [tool_create_directory]  
            response = client.chat.completions.create(
                model="gpt-3.5-turbo-16k",
                messages=messages,
                tools=tools,
                tool_choice="auto",
            )
            response_message = response.choices[0].message
            tool_calls = response_message.tool_calls
            # Check if the model called a function
            if tool_calls:
                # Proceed to step 3
        ```
        ---
        # Step 3: Parse and execute the function

        ```python
          available_functions = {
            "create_directory": create_directory,
        }
        messages.append(response_message)
        for tool_call in tool_calls:
            function_name = tool_call.function.name
            function_to_call = available_functions[function_name]
            function_args = json.loads(tool_call.function.arguments)
            function_response = function_to_call(
                directory_name=function_args.get("directory_name"),
            )
            messages.append(
                {
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": function_name,
                    "content": function_response,
                }
            )
        ```

        ---
        # Step 4: Summarize Results Back to User

        ```python
            second_response = client.chat.completions.create(
              model="gpt-3.5-turbo-16k",
              messages=messages,
          )
          return second_response

          output = run_terminal_task()

        ```

        ---
        class: center, middle

        # Ready to Implement Your Own Functions?

        <h2><span style="background-color: lightgreen">
          OpenAI Functions Demo
        </span> </h2>

        ---
        class: center, middle

        # Break 10 Minutes

        ---
        class: center, middle

        # Agents and AutoGen

        ---
        # How Can We Effectively Perform Tasks with Agents?

        --
  
        <img style="width: 700px" src="../notebooks/assets-resources/agent_loop.png">

        - [The Agent Loop](https://blog.langchain.dev/openais-bet-on-a-cognitive-architecture/#:~:text=sweep.dev%20is%20another%20great%20example.%20they%20wrote%20a%20blog%20over%20the%20summer%20describing%20their%20cognitive%20architecture%2C%20including%20a%20fantastic%20diagram.)

        <p style="font-size: 14px; margin-top: 20px; margin-left: 400px;">
           <a href="https://blog.langchain.dev/openais-bet-on-a-cognitive-architecture/#:~:text=sweep.dev%20is%20another%20great%20example.%20they%20wrote%20a%20blog%20over%20the%20summer%20describing%20their%20cognitive%20architecture%2C%20including%20a%20fantastic%20diagram.">OpenAI's Bet on a Cognitive Architecture</a>
        </p>

        ---
        # Good Agents Colaborate

        <img style="width: 650px" src="../notebooks/assets-resources/agents-collab-loop.svg">

        --

        - With the increasing complexity of tasks, agents need to __collaborate__ to achieve their goals

        --

        - AutoGen is a framework that facilitates the creation of agents that can easily collaborate through a 'conversation-centric' paradigm 


        ---

        # What is AutoGen?

        --

        - [AutoGen](https://microsoft.github.io/autogen/) is a framework for building agents that can collaborate
        through conversational patterns to accomplish tasks

        --

        - Its main features

        --

          - __Conversable agents__: a generic design of agents 
          that can leverage LLMs, human input, tools or a combination of these 
          to facilitate creating agents with different roles
        --
        

          - __Conversation Programming__: Programming paradigm centered around inter-agent conversations
        
        ---

        # Conversable Agents

        --

        - __Conversable__: Entity with a specific role that can pass messages to send and
        receive information to and from other conversable agents, e.g., to start or continue a conversation.

        --

        - __Customizable__: Based on application-specific needs, each agent can be
        configured to have a mix of basic back-end types to display complex behavior in multi-agent conversations.

        --

        - __Example__:

          ```python
          assistant = autogen.AssistantAgent(
                        name="assistant",
                        llm_config=llm_config,
                        )
          ```
        
        ---
        # Conversation Programming
        
        ![](../notebooks/assets-resources/conversation_programming.png)
        

        ---
        class: center, middle

        # Practicing the Basics of AutoGen
        <h2><span style="background-color: lightgreen">
          AutoGen Demo
        </span> </h2>

        ---
        class: center, middle

        # Break

        ---
        
        # Building Agents with AutoGen

        --

        ## The Agent Loop 

        --

        <img src="../notebooks/assets-resources/agent_loop.svg" width="800">

        ---
        # Key LangChain Components for Agents

        ## Schema

        --

        - LangChain provides many abstractions for ease of use

        --

        - **AgentAction**: Represents the action an agent should take.

        --

        - **AgentFinish**: Represents the final result to return to the user.
        
        --

        - **Intermediate Steps**: Previous actions and outputs for the current agent run.

        --

        - **Agent**: Chain responsible for deciding the next step, powered by a language model.

        ---

        # Agent Inputs and Outputs

        ## Agent Inputs

        --
        
        - Key-value mapping.

        --

        - Required key: `intermediate_steps`.

        --

        ## Agent Outputs

        --

        - Next actions or final response (AgentActions or AgentFinish).

        --

        - Handled by the output parser.

        ---
        # AgentExecutor

        ## The runtime for a LangChain agent

        ```python
        next_action = agent.get_action(...)
        while next_action != AgentFinish:
            observation = run(next_action)
            next_action = agent.get_action(..., next_action, observation)
        return next_action
        ```

        - Handles complexities like tool errors and logging.

        ---
        class: center, middle

        <h2><span style="background-color: lightgreen">
          LangChain Agents Demo
        </span> </h2>

        ---
        class: center, middle

        # Break

        ---

        # Tools in LangChain

        --

        - Functions that an agent can invoke.

        --

        - Consists of:

        --
          - Input schema for the tool.

        --

          - Function to run.

        --

        - Important for building a working agent.
        ---

        # Toolkits

        - Groups of 3-5 tools for specific objectives.

        --

        - Example: GitHub toolkit for interacting with GitHub.

        --

        - LangChain provides a wide set of toolkits.

        ---
        class: center, middle

        # Let's Build Agents!

        <h2><span style="background-color: lightgreen">
          LangChain Agents Demo - Github Agent; Tutor Agent; Research Assistant
        </span> </h2>

        ---

        # References

        - [HuggingGPT](https://github.com/microsoft/JARVIS)
        - [Gen Agents](https://arxiv.org/pdf/2304.03442.pdf)
        - [WebGPT](https://www.semanticscholar.org/paper/WebGPT%3A-Browser-assisted-question-answering-with-Nakano-Hilton/2f3efe44083af91cef562c1a3451eee2f8601d22)
        - [LangChain](https://python.langchain.com/docs/get_started/introduction)
        - [OpenAI](https://openai.com/)
        - [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)
        - [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
        - [GPT-Engineer](https://github.com/gpt-engineer-org/gpt-engineer)
        - [BabyAGI](https://github.com/yoheinakajima/babyagi)
        - [Karpathy on Agents](https://www.youtube.com/watch?v=fqVLjtvWgq8)
        - [ReACT Paper](https://arxiv.org/abs/2210.03629)

        ---

        class: center, middle

        # Conclusion
        ## Ready to Build Your Own Agents?        

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>


